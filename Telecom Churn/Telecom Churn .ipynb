{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Telecom Churn : Prediction</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement<br>\n",
    "Analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.<br>\n",
    "In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. The telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "## Goal\n",
    "- Analyse customer data with **Prepaid** pland in **India and Southest Asia**\n",
    "- Identify High-Value customer based on there uses\n",
    "    - Take first 2 month average recharge amount\n",
    "    - Identify customer with more than **70<sup>th</sup> Percentile** average recharge amount \n",
    "- Here we have 4 month data (Jun, July, August, September)\n",
    "    - Tag churner in the last month (September) using fourth month data\n",
    "    - Based on usage of fourth month such as incoming and outgoing call and internet use\n",
    "    - After prediction remove all the attributes corresponding to the churn phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import datetime as dt\n",
    "import warnings\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def customprint(text):\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max.columns', 250)\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custDf = pd.read_csv(\"telecom_churn_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custDf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#changing few column names\n",
    "custDf.rename(columns={'jun_vbc_3g':'vbc_3g_6', 'jul_vbc_3g':'vbc_3g_7', 'aug_vbc_3g':'vbc_3g_8', 'sep_vbc_3g':'vbc_3g_9'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indentify **High-Value** customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for different recharge amount and recharge count\n",
    "custDf[[c for c in custDf.columns if ((c.endswith('_6')) \n",
    "                                      & (('rch' in c) | \n",
    "                                         ('rech' in c) | \n",
    "                                         ('sachet' in c) | \n",
    "                                         ('monthly' in c) | \n",
    "                                         ('night' in c) | \n",
    "                                         ('VBC' in c)))]].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custDf[['total_rech_num_6','total_rech_data_6', 'count_rech_2g_6', 'count_rech_3g_6', 'sachet_2g_6', 'sachet_3g_6','monthly_2g_6','sachet_2g_6','monthly_3g_6']][~np.isnan(custDf['total_rech_data_6'])].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custDf[['total_rech_amt_6','max_rech_amt_6', 'max_rech_data_6', 'av_rech_amt_data_6']][~np.isnan(custDf['total_rech_data_6'])].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing value : Average Recharge Amount for data (June) : \", custDf[np.isnan(custDf['av_rech_amt_data_6']) & custDf['total_rech_data_6'] > 0].size)\n",
    "print(\"Missing value : Average Recharge Amount for data (July) : \", custDf[np.isnan(custDf['av_rech_amt_data_7']) & custDf['total_rech_data_7'] > 0].size)\n",
    "print(\"Missing value : Average Recharge Amount for data (August) : \", custDf[np.isnan(custDf['av_rech_amt_data_8']) & custDf['total_rech_data_8'] > 0].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Calculate Total recharge amount spned on data for specific month\n",
    "custDf['total_rech_amt_data_6'] = custDf['av_rech_amt_data_6'] * custDf['total_rech_data_6']\n",
    "custDf['total_rech_amt_data_7'] = custDf['av_rech_amt_data_7'] * custDf['total_rech_data_7']\n",
    "custDf['total_rech_amt_data_8'] = custDf['av_rech_amt_data_8'] * custDf['total_rech_data_8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custDf['total_rech_6'] = custDf[['total_rech_amt_data_6', 'total_rech_amt_6']].sum(axis=1)\n",
    "custDf['total_rech_7'] = custDf[['total_rech_amt_data_7', 'total_rech_amt_7']].sum(axis=1)\n",
    "custDf['total_rech_8'] = custDf[['total_rech_amt_data_8', 'total_rech_amt_8']].sum(axis=1)\n",
    "custDf['av_rech_6_7'] = custDf[['total_rech_6','total_rech_7']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#70th percentile of two month average recharge  \n",
    "rechAmt = custDf['av_rech_6_7'].quantile(0.7)\n",
    "rechAmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#High value customer with average recharge amount greater than 70 percentile\n",
    "hvCust = custDf[custDf['av_rech_6_7'] >= rechAmt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvCust.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will drop 'av_rech_6_7' columns which might create confusion in EDA process\n",
    "hvCust.drop(labels=['av_rech_6_7'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag churner based on Incomeing & Outgoing call as well as Internet usages in september month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvCust['churn'] = hvCust.apply(lambda x : 1 if ((x['total_ic_mou_9'] == 0) & (x['total_og_mou_9'] == 0) & (x['vol_2g_mb_9'] == 0) & (x['vol_3g_mb_9'] == 0)) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvCust[['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9', 'churn']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop attribute from churn phase\n",
    "colsToDel = [c for c in hvCust.columns if \"_9\" in c]\n",
    "hvCust.drop(labels=colsToDel, inplace=True, axis=1)\n",
    "hvCust.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as name suggeted \"Last Date of Month\" column contain unique value (Last day of that month)\n",
    "#So we can drop these 3 columns\n",
    "hvCust.drop(labels=['last_date_of_month_6','last_date_of_month_7','last_date_of_month_8'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvCust.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch day from last recharge date of specific month\n",
    "hvCust['date_of_last_rech_6'] = pd.DatetimeIndex(hvCust['date_of_last_rech_6']).day\n",
    "hvCust['date_of_last_rech_7'] = pd.DatetimeIndex(hvCust['date_of_last_rech_7']).day\n",
    "hvCust['date_of_last_rech_8'] = pd.DatetimeIndex(hvCust['date_of_last_rech_8']).day\n",
    "hvCust['date_of_last_rech_data_6'] = pd.DatetimeIndex(hvCust['date_of_last_rech_data_6']).day\n",
    "hvCust['date_of_last_rech_data_7'] = pd.DatetimeIndex(hvCust['date_of_last_rech_data_7']).day\n",
    "hvCust['date_of_last_rech_data_8'] = pd.DatetimeIndex(hvCust['date_of_last_rech_data_8']).day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvCust.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missingDf = pd.DataFrame(data=hvCust.isnull().sum() / hvCust.index.size * 100, columns=['MissingPercent'])\n",
    "missingDf =  missingDf[missingDf['MissingPercent'] > 0]\n",
    "missingDf.reset_index(inplace=True)\n",
    "missingDf.columns = ['Feature', 'MissingPercent']\n",
    "missingDf[['Month', 'Feature']] = missingDf['Feature'].apply(lambda x : pd.Series([6, x.replace('_6', '')] if x.endswith('_6') else ([7, x.replace('_7', '')] if x.endswith('_7') else ([8, x.replace('_8', '')] if x.endswith('_8') else [None, x]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pvtDf =  missingDf[~np.isnan(missingDf['MissingPercent'])].pivot_table(index=['Feature'], columns=['Month'])\n",
    "pvtDf['MissingPercent'].sort_values(by=[6.0], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hvCust[hvCust['total_rech_amt_6'] == 0][[c for c in hvCust.columns if '_6' in c]]\n",
    "hvCust[np.isnan(hvCust['total_rech_data_6']) | \n",
    "      np.isnan(hvCust['total_rech_amt_data_6']) |\n",
    "      np.isnan(hvCust['night_pck_user_6']) |\n",
    "      np.isnan(hvCust['max_rech_data_6']) |\n",
    "      np.isnan(hvCust['fb_user_6']) |\n",
    "      np.isnan(hvCust['count_rech_3g_6']) |\n",
    "      np.isnan(hvCust['count_rech_2g_6']) |\n",
    "      np.isnan(hvCust['av_rech_amt_data_6']) |\n",
    "      np.isnan(hvCust['arpu_3g_6']) |\n",
    "      np.isnan(hvCust['arpu_2g_6'])][[c for c in hvCust.columns if '_6' in c]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from **date_of_last_rech** column all other column have some common pattern.<BR>\n",
    "All feature variable related to Internet Service have **44% missing data in Jun, 43% in July and 47% in August ** month.<BR>\n",
    "Same as internet service, calling service related features also have common trend.<BR>\n",
    "\n",
    "This trend shows there are few customers only use calling service and not using any internet service, hence approx. 44% missing data for all columns of internet service related feature.<BR>\n",
    "We can impute zero in such columns except last recharge date data column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvCust.drop(labels=['date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = pvtDf[pvtDf['MissingPercent'][6] > 40].index\n",
    "cols = [c + \"_\" + month for c in cols for month in ['6','7','8']]\n",
    "cols.remove('date_of_last_rech_data_6')\n",
    "cols.remove('date_of_last_rech_data_7')\n",
    "cols.remove('date_of_last_rech_data_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvCust[cols] = hvCust[cols].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop rows of other missing data\n",
    "hvCust.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "missingDf = pd.DataFrame(data=hvCust.isnull().sum() / hvCust.index.size * 100, columns=['MissingPercent'])\n",
    "missingDf =  missingDf[missingDf['MissingPercent'] > 0]\n",
    "missingDf.reset_index(inplace=True)\n",
    "missingDf.columns = ['Feature', 'MissingPercent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingDf.sort_values(by='MissingPercent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hvCust.shape)\n",
    "print(hvCust['churn'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per defination **Good Phase** (i.e. June & July), customer is happy with service provider and in **Action Phase** (i.e. August) it shows show different behaviour.<BR>\n",
    "So will combine June & July data points for analsysi and futher use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will exclude few variable such as categorical (0,1) and related to date/day\n",
    "colToExclude = ['date_of_last_reach_6','date_of_last_reach_7','fb_user_6','fb_user_7','night_pck_user_6','night_pck_user_7']\n",
    "for col in hvCust.columns:\n",
    "    if (col.endswith('_6')) & (col not in colToExclude):\n",
    "        hvCust[col + '_7'] = hvCust[[col, col.strip('_6') + '_7']].mean(axis=1)\n",
    "        hvCust.drop(labels=[col, col.strip('_6') + '_7'], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duplicate data check\n",
    "hvCust[hvCust['mobile_number'].duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvCust.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few feature variables contain only **0** value, we can drop such variables.<BR>\n",
    "Also **circle_id** feature have common value **109**, we can drop this feature also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hvCust.index.size\n",
    "tempDf = pd.DataFrame([[c, hvCust[hvCust[c] == 0].index.size / s] for c in hvCust.columns if (hvCust[hvCust[c] == 0].index.size / s > 0.7)])\n",
    "tempDf.columns = ['Feature', '% of 0']\n",
    "tempDf.sort_values(by=['% of 0'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will drop features having more than 98% zero values\n",
    "colToDrop = list(tempDf[tempDf['% of 0'] > 0.98]['Feature'])\n",
    "colToDrop.append('circle_id')\n",
    "hvCust.drop(labels=colToDrop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Correlation of Good Phase Data\n",
    "col = [c for c in hvCust.columns if c.endswith('_6_7') ] + ['churn']\n",
    "corrData_6_7 = hvCust[col].corr()\n",
    "plt.figure(figsize=(15,12))\n",
    "mask = np.zeros_like(corrData_6_7, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corrData_6_7, cmap='RdBu', mask=mask, center=0, linewidths= 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation of Action Phase Data\n",
    "col = [c for c in hvCust.columns if c.endswith('_8') ] + ['churn']\n",
    "corrData_8 = hvCust[col].corr()\n",
    "plt.figure(figsize=(15,12))\n",
    "mask = np.zeros_like(corrData_8, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(corrData_8, cmap='RdBu', mask=mask, center=0, linewidths= 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both heatmap shows similar trend, will drop few highly correlated column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corrData_6_7.columns:\n",
    "    corrCol = corrData_6_7[(corrData_6_7[c] >= 0.8) & (corrData_6_7[c] < 1)].index\n",
    "    if (corrCol.size > 0):\n",
    "        print(\"'{0}' correlated with '{1}' : {2}\".format(c, corrCol[0], corrData_6_7.loc[c, corrCol[0]]))\n",
    "print()        \n",
    "print('-----------------------------------------------------------------')\n",
    "print()\n",
    "for c in corrData_8.columns:\n",
    "    corrCol = corrData_8[(corrData_8[c] >= 0.8) & (corrData_8[c] < 1)].index\n",
    "    if (corrCol.size > 0):\n",
    "        print(\"'{0}' correlated with '{1}' : {2}\".format(c, corrCol[0], corrData_8.loc[c, corrCol[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's drop highly correlated Columns\n",
    "colToDrop = ['total_rech_amt_6_7','total_rech_amt_8','std_og_t2t_mou_6_7','std_og_t2t_mou_8','std_og_t2m_mou_6_7','std_og_t2m_mou_8','std_og_mou_6_7','std_og_mou_8','loc_ic_mou_6_7','loc_ic_mou_8','std_ic_t2m_mou_6_7','std_ic_t2m_mou_8','total_rech_data_6_7','total_rech_data_8','sachet_3g_6_7','sachet_3g_8','sachet_2g_6_7','sachet_2g_8']\n",
    "hvCust.drop(labels=colToDrop, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvCust.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['fb_user_6','fb_user_7','fb_user_8','churn']\n",
    "n_cols = 4\n",
    "n_rows = math.ceil(len(cols) / n_cols)\n",
    "plt.figure(figsize=(12,4))\n",
    "for i,c in enumerate(sorted(cols)):\n",
    "    plt.subplot(n_rows, n_cols, i+1)\n",
    "    ax = sns.countplot(x=hvCust[c])\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "\n",
    "    plt.title(c.capitalize(), y=1, fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = hvCust.columns[~hvCust.columns.isin(['mobile_number','fb_user_6','fb_user_7','fb_user_8','churn'])]\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(len(cols) / n_cols)\n",
    "plt.figure(figsize=(15,100))\n",
    "for i,c in enumerate(sorted(cols)):\n",
    "    plt.subplot(n_rows, n_cols, i+1)\n",
    "    ax = sns.distplot(hvCust[c])\n",
    "    ax.set_xlabel('')\n",
    "    \n",
    "    plt.title(c.capitalize(), y=0.85, fontsize=15)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All feature variables are either skewed to right or left, not a single variable showing normal distribution.<BR>\n",
    "We are analyzing high value customers which is nothing but outliers, so we can expect such trend in all other variable. Hence instead of removing outlier data will start building model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "#Util\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold\n",
    "\n",
    "#Model Algo\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#Model Evluation\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, recall_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train & Test Data\n",
    "Y = hvCust['churn']\n",
    "X = hvCust.drop(labels=['churn','mobile_number'], axis=1)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.3, random_state = 100, stratify = Y)\n",
    "print(\"Train Set :\", X_train.shape, Y_train.shape)\n",
    "print(\"Test Set :\", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data using Standard Scaler\n",
    "sc = StandardScaler()\n",
    "X_train[X_train.columns] = sc.fit_transform(X_train[X_train.columns])\n",
    "X_test[X_test.columns] = sc.transform(X_test[X_test.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of building model on all feature will fetch important feature using RFE\n",
    "rfe = RFE(LogisticRegression(), n_features_to_select=60)\n",
    "rfe.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rfeDF = pd.DataFrame(data=np.array([X_train.columns, rfe.support_, rfe.ranking_]).T, columns=['Feature', 'Support', 'Ranking'])\n",
    "rfeDF[rfeDF['Support']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(svd_solver='randomized', random_state=100)\n",
    "pca.fit(X_train[rfeDF[rfeDF['Support']]['Feature']])\n",
    "#pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.yticks(np.arange(0,1.1,0.1))\n",
    "plt.grid(linestyle='-', linewidth = 0.5)\n",
    "plt.xlabel('Principle Components')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)[np.arange(29,60,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(pca.explained_variance_ratio_)#[np.arange(30,60,5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**91%** variance explained by **30** PC<BR>\n",
    "**94%** variance explained by **35** PC<BR>\n",
    "**96%** variance explained by **40** PC<BR>\n",
    "**98%** variance explained by **45** PC<BR>\n",
    "**99%** variance explained by **50** PC<BR>\n",
    "**100%** variance explained by **55** PC<BR>\n",
    "So, we can choose **40 or 45** PC for model building which explained **96% - 98%** variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = IncrementalPCA(n_components=40)\n",
    "pca_train = pca.fit_transform(X_train[rfeDF[rfeDF['Support']]['Feature']])\n",
    "pca_test = pca.transform(X_test[rfeDF[rfeDF['Support']]['Feature']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, random_state = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "param = {'C' : [1,10,100,1000]}\n",
    "model_cv = GridSearchCV(lr, param_grid=param, scoring='recall', cv=folds, n_jobs=-1, verbose=1, return_train_score=True)\n",
    "model_cv.fit(pca_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrScore = pd.DataFrame(model_cv.cv_results_)\n",
    "lrScore[['param_C','mean_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 100)\n",
    "param = {'C' : [0.001,0.1,1]}\n",
    "svc = SVC(kernel='linear')\n",
    "model_scv_li = GridSearchCV(svc,\n",
    "                             param_grid=param, \n",
    "                             scoring='precision', \n",
    "                             cv=folds, \n",
    "                             verbose=1,\n",
    "                             n_jobs = -1,\n",
    "                             return_train_score=True)\n",
    "model_scv_li.fit(pca_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scv_li_Score = pd.DataFrame(model_scv_li.cv_results_)\n",
    "model_scv_li_Score[['param_C','mean_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='linear')\n",
    "param = {'C' : [0.01, 0.1, 1, 10]}\n",
    "#param = {'C' : [0.1]}\n",
    "model_svc_lr = GridSearchCV(svc, param_grid=param, cv=2, scoring='accuracy', n_jobs=-1, verbose=1, return_train_score=True)\n",
    "model_svc_lr.fit(pca_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scvlrScore = pd.DataFrame(model_svc_lr.cv_results_)\n",
    "scvlrScore[['param_C','mean_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "param = {'C' : [0.01, 0.1, 1, 10]}\n",
    "svc = SVC(kernel='linear')\n",
    "model_scv_li = GridSearchCV(svc,\n",
    "                             param_grid=param, \n",
    "                             scoring='accuracy', \n",
    "                             cv=folds, \n",
    "                             verbose=1,\n",
    "                             n_jobs = -1,\n",
    "                             return_train_score=True)\n",
    "model_scv_li.fit(pca_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='poly')\n",
    "param = {'C' : [0.01, 0.1, 1, 10],\n",
    "        'gamma': [0.01, 0.1, 1]}\n",
    "model_svc_poly = GridSearchCV(svc, param_grid=param, cv=folds, scoring='recall', n_jobs=-1, return_train_score=True)\n",
    "model_svc_poly.fit(pca_train, Y_train)\n",
    "\n",
    "scvpolyScore = pd.DataFrame(model_svc_poly.cv_results_)\n",
    "scvpolyScore[['param_C','mean_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = SVC(kernel='rbf')\n",
    "param = {'C' : [0.01, 0.1, 1, 10],\n",
    "        'gamma': [0.01, 0.1, 1]}\n",
    "model_svc_rbf = GridSearchCV(svc, param_grid=param, cv=folds, scoring='recall', n_jobs=-1, return_train_score=True)\n",
    "model_svc_rbf.fit(pca_train, Y_train)\n",
    "\n",
    "scvrbfScore = pd.DataFrame(model_svc_rbf.cv_results_)\n",
    "scvrbfScore[['param_C','mean_test_score','mean_train_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
